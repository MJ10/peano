task: train

domain: equations
number_of_episodes: 100000
save_every: 1000
max_steps: 20
max_state_length: 1000
output_path: "data/pretraining-episodes-equations.{}.pkl"

dataset: "/mnt/fs3/poesia/peano/learning/data/pretraining-episodes-equations.{}.pkl"
shard: 0
shards: 20
n_augmentations: 2
batch_size: 2000
gradient_steps: 1000000
checkpoint_every: 10000
goal_mask_probability: 0.05
learning_rate: 0.00005

policy:
    type: DecisionTransformer
    batch_size: 2000
    discard_unsolved: true
    mask_non_decision_tokens: false
    max_negatives: 5

    reformer:
        feed_forward_size: 256
        hidden_size: 256
        axial_pos_embds_dim: [64, 192]
        axial_pos_shape: [64, 64]
        num_attention_heads: 6
        num_hidden_layers: 6


job:
    description: "Pre-training job"
    wandb_project: "peano"

hydra:
    job:
        chdir: false
