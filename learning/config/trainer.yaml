task: train

train_interval: [500, 1000000]
eval_interval: [0, 100]

trainer:
    train_domains:
        - mix(subst-eval, comb-like, one-step-add-eq, one-step-mul-eq, two-step-eq)
        # - subst-eval

    eval_domains:
        - subst-eval
        - comb-like
        - one-step-add-eq
        - one-step-mul-eq
        - two-step-eq

    passing_grade: 0.9
    adjust_search_budget_threshold: -1
    search_budget_multiplier: 1
    do_eval: false
    on_policy_frac: 1.
    eval_freq: 250
    iterations: 500
    batch_size: 50
    n_searchers: 12
    max_depth: 18
    max_nodes: 1000
    rerank_top_k: 200
    epsilon: 0.1
    algorithm: 'policy-beam-search'
    accumulate: true
    load_tactics_seq: null
    load_tactics_init: null
    load_trajectories: null
    do_on_policy_eval: false
    gpus: [0, 0]
    induce_tactics: true
    n_tactics: 3
    min_tactic_score: 50
    load_past_exp_trajectories: null

    model:
        type: contrastive-policy
        discard_unsolved: true
        train_value_function: false
        solution_augmentation_probability: 0.5
        solution_augmentation_rate: 0.5

        gru:
            embedding_size: 64
            hidden_size: 256
            layers: 2

        gradient_steps: 2048
        batch_size: 1
        lr: 0.001

        interaction: dot-product
        normalize: true

job:
    wandb_project: peano
    name: test_exp

hydra:
    run:
        dir: /network/scratch/m/moksh.jain/peano_experiments/${job.name}/${now:%Y.%m.%d}/${now:%H%M%S}
    job:
        chdir: true
